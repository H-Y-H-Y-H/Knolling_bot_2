this is para_dict
 {'device': 'cuda:0', 'num_img': 200000, 'ratio': 0.8, 'epoch': 300, 'model_path': './results/MLP_907_1/', 'input_data_path': '../../knolling_dataset/MLP_unstack_907_intensive/labels_box/', 'output_data_path': '../../knolling_dataset/MLP_unstack_907_intensive/labels_unstack/', 'learning_rate': 0.001, 'patience': 10, 'factor': 0.1, 'batch_size': 64, 'output_size': 6, 'abort_learning': 20, 'set_dropout': 0.05, 'num_boxes': 5, 'run_name': 'MLP_907_1', 'project_name': 'zzz_MLP_unstack', 'wandb_flag': True, 'use_mse': True, 'use_scaler': False, 'fine-tuning': False, 'node_1': 128, 'node_2': 32, 'node_3': 8}
load the train data ...








100%|██████████| 160000/160000 [00:18<00:00, 8844.96it/s]
 36%|███▌      | 14209/40000 [00:01<00:02, 8855.58it/s]
total train data: 160000


100%|██████████| 40000/40000 [00:04<00:00, 8865.56it/s]
total valid data: 40000
not fine-tuning
Training_Loss At Epoch 0:	 0.361086
Testing_Loss At Epoch 0:	 0.35681
epoch0, time used: 3.91, lr: 0.001
epoch1, time used: 3.51, lr: 0.001
Training_Loss At Epoch 2:	 0.356437
Testing_Loss At Epoch 2:	 0.35465
epoch2, time used: 3.56, lr: 0.001
Training_Loss At Epoch 3:	 0.354742
Testing_Loss At Epoch 3:	 0.352844
epoch3, time used: 3.49, lr: 0.001
Training_Loss At Epoch 4:	 0.352839
Testing_Loss At Epoch 4:	 0.352403
epoch4, time used: 3.3, lr: 0.001
Training_Loss At Epoch 5:	 0.351163
Testing_Loss At Epoch 5:	 0.3517
epoch5, time used: 3.66, lr: 0.001
Training_Loss At Epoch 6:	 0.349239
Testing_Loss At Epoch 6:	 0.349946
epoch6, time used: 3.57, lr: 0.001
Training_Loss At Epoch 7:	 0.347786
Testing_Loss At Epoch 7:	 0.347921
epoch7, time used: 3.43, lr: 0.001
epoch8, time used: 3.44, lr: 0.001
epoch9, time used: 3.54, lr: 0.001
epoch10, time used: 3.59, lr: 0.001
Training_Loss At Epoch 11:	 0.341858
Testing_Loss At Epoch 11:	 0.347098
epoch11, time used: 3.38, lr: 0.001
Training_Loss At Epoch 12:	 0.341261
Testing_Loss At Epoch 12:	 0.346247
epoch12, time used: 3.37, lr: 0.001
epoch13, time used: 3.43, lr: 0.001
epoch14, time used: 3.36, lr: 0.001
epoch15, time used: 3.32, lr: 0.001
epoch16, time used: 3.46, lr: 0.001
epoch17, time used: 3.61, lr: 0.001
epoch18, time used: 3.41, lr: 0.001
epoch19, time used: 3.61, lr: 0.001
epoch20, time used: 3.46, lr: 0.001
epoch21, time used: 3.36, lr: 0.001
epoch22, time used: 3.59, lr: 0.001
epoch23, time used: 3.62, lr: 0.001
epoch24, time used: 3.44, lr: 0.0001
epoch25, time used: 3.66, lr: 0.0001
epoch26, time used: 3.5, lr: 0.0001
epoch27, time used: 3.47, lr: 0.0001
epoch28, time used: 3.43, lr: 0.0001
epoch29, time used: 3.56, lr: 0.0001
epoch30, time used: 3.43, lr: 0.0001
epoch31, time used: 3.44, lr: 0.0001
epoch32, time used: 3.64, lr: 0.0001
epoch33, time used: 3.59, lr: 0.0001