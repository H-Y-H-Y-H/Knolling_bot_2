this is para_dict
 {'device': 'cuda:0', 'num_img': 160000, 'ratio': 0.8, 'epoch': 300, 'model_path': './results/MLP_905_2/', 'input_data_path': '../../knolling_dataset/MLP_unstack_905/labels_box/', 'output_data_path': '../../knolling_dataset/MLP_unstack_905/labels_unstack/', 'learning_rate': 0.001, 'patience': 10, 'factor': 0.1, 'batch_size': 64, 'output_size': 6, 'abort_learning': 20, 'set_dropout': 0.05, 'num_boxes': 5, 'run_name': 'MLP_905_2', 'project_name': 'zzz_MLP_unstack', 'wandb_flag': True, 'use_mse': True, 'use_scaler': False, 'fine-tuning': False, 'node_1': 128, 'node_2': 32, 'node_3': 8}
load the train data ...






 96%|█████████▌| 122491/128000 [00:13<00:00, 9193.20it/s]
total train data: 128000
100%|██████████| 128000/128000 [00:14<00:00, 9005.45it/s]

 98%|█████████▊| 31449/32000 [00:03<00:00, 9233.13it/s]
total valid data: 32000

100%|██████████| 32000/32000 [00:03<00:00, 9228.73it/s]
Training_Loss At Epoch 0:	 0.373283
Testing_Loss At Epoch 0:	 0.364794
epoch0, time used: 3.09, lr: 0.001
Training_Loss At Epoch 1:	 0.354609
Testing_Loss At Epoch 1:	 0.346599
epoch1, time used: 2.63, lr: 0.001
Training_Loss At Epoch 2:	 0.339445
Testing_Loss At Epoch 2:	 0.337079
epoch2, time used: 2.64, lr: 0.001
Training_Loss At Epoch 3:	 0.333949
Testing_Loss At Epoch 3:	 0.334425
epoch3, time used: 2.6, lr: 0.001
Training_Loss At Epoch 4:	 0.332151
Testing_Loss At Epoch 4:	 0.332964
epoch4, time used: 2.65, lr: 0.001
epoch5, time used: 2.66, lr: 0.001
epoch6, time used: 2.65, lr: 0.001
epoch7, time used: 2.64, lr: 0.001
Training_Loss At Epoch 8:	 0.329437
Testing_Loss At Epoch 8:	 0.33214
epoch8, time used: 2.65, lr: 0.001
epoch9, time used: 2.67, lr: 0.001
Training_Loss At Epoch 10:	 0.328569
Testing_Loss At Epoch 10:	 0.329576
epoch10, time used: 2.64, lr: 0.001
Training_Loss At Epoch 11:	 0.32801
Testing_Loss At Epoch 11:	 0.329035
epoch11, time used: 2.61, lr: 0.001
epoch12, time used: 2.65, lr: 0.001
epoch13, time used: 2.63, lr: 0.001
epoch14, time used: 2.62, lr: 0.001
epoch15, time used: 2.63, lr: 0.001
Training_Loss At Epoch 16:	 0.326103
Testing_Loss At Epoch 16:	 0.32792
epoch16, time used: 2.65, lr: 0.001
Training_Loss At Epoch 17:	 0.325834
Testing_Loss At Epoch 17:	 0.327818
epoch17, time used: 2.66, lr: 0.001
epoch18, time used: 2.66, lr: 0.001
Training_Loss At Epoch 19:	 0.325144
Testing_Loss At Epoch 19:	 0.327526
epoch19, time used: 2.67, lr: 0.001
epoch20, time used: 2.65, lr: 0.001
Training_Loss At Epoch 21:	 0.324477
Testing_Loss At Epoch 21:	 0.327228
epoch21, time used: 2.66, lr: 0.001
epoch22, time used: 2.65, lr: 0.001
epoch23, time used: 2.64, lr: 0.001
epoch24, time used: 2.63, lr: 0.001
Training_Loss At Epoch 25:	 0.32383
Testing_Loss At Epoch 25:	 0.326643
epoch25, time used: 2.65, lr: 0.001
epoch26, time used: 2.66, lr: 0.001
epoch27, time used: 2.63, lr: 0.001
epoch28, time used: 2.66, lr: 0.001
epoch29, time used: 2.66, lr: 0.001
epoch30, time used: 2.49, lr: 0.001
Training_Loss At Epoch 31:	 0.322949
Testing_Loss At Epoch 31:	 0.325912
epoch31, time used: 2.44, lr: 0.001
Training_Loss At Epoch 32:	 0.322928
Testing_Loss At Epoch 32:	 0.324828
epoch32, time used: 2.56, lr: 0.001
epoch33, time used: 2.66, lr: 0.001
epoch34, time used: 2.63, lr: 0.001
epoch35, time used: 2.63, lr: 0.001
epoch36, time used: 2.65, lr: 0.001
epoch37, time used: 2.63, lr: 0.001
Training_Loss At Epoch 38:	 0.32167
Testing_Loss At Epoch 38:	 0.324368
epoch38, time used: 2.62, lr: 0.001
epoch39, time used: 2.63, lr: 0.001
epoch40, time used: 2.61, lr: 0.001
Training_Loss At Epoch 41:	 0.32133
Testing_Loss At Epoch 41:	 0.324207
epoch41, time used: 2.63, lr: 0.001
epoch42, time used: 2.63, lr: 0.001
Training_Loss At Epoch 43:	 0.321301
Testing_Loss At Epoch 43:	 0.323695
epoch43, time used: 2.4, lr: 0.001
epoch44, time used: 2.64, lr: 0.001
epoch45, time used: 2.63, lr: 0.001
epoch46, time used: 2.63, lr: 0.001
epoch47, time used: 2.34, lr: 0.001
epoch48, time used: 2.62, lr: 0.001
epoch49, time used: 2.64, lr: 0.001
epoch50, time used: 2.63, lr: 0.001
epoch51, time used: 2.63, lr: 0.001
epoch52, time used: 2.55, lr: 0.001
epoch53, time used: 2.62, lr: 0.001
epoch54, time used: 2.36, lr: 0.001
Training_Loss At Epoch 55:	 0.316389
Testing_Loss At Epoch 55:	 0.322791
epoch55, time used: 2.6, lr: 0.0001
Training_Loss At Epoch 56:	 0.315795
Testing_Loss At Epoch 56:	 0.322685
epoch56, time used: 2.67, lr: 0.0001
Training_Loss At Epoch 57:	 0.315611
Testing_Loss At Epoch 57:	 0.322611
epoch57, time used: 2.65, lr: 0.0001
epoch58, time used: 2.63, lr: 0.0001
epoch59, time used: 2.63, lr: 0.0001
epoch60, time used: 2.63, lr: 0.0001
epoch61, time used: 2.59, lr: 0.0001
epoch62, time used: 2.64, lr: 0.0001
epoch63, time used: 2.62, lr: 0.0001
epoch64, time used: 2.62, lr: 0.0001
epoch65, time used: 2.57, lr: 0.0001
epoch66, time used: 2.61, lr: 0.0001
epoch67, time used: 2.5, lr: 0.0001
epoch68, time used: 2.63, lr: 0.0001
epoch69, time used: 2.64, lr: 1e-05
epoch70, time used: 2.49, lr: 1e-05
epoch71, time used: 2.67, lr: 1e-05
epoch72, time used: 2.67, lr: 1e-05
epoch73, time used: 2.65, lr: 1e-05
epoch74, time used: 2.52, lr: 1e-05
epoch75, time used: 2.68, lr: 1e-05
epoch76, time used: 2.72, lr: 1e-05
epoch77, time used: 2.6, lr: 1e-05
epoch78, time used: 2.62, lr: 1e-05