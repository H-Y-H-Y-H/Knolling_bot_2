
 37%|███▋      | 14148/38400 [00:01<00:02, 8697.01it/s]
this is para_dict
 {'device': 'cuda:0', 'num_img': 48000, 'ratio': 0.8, 'epoch': 300, 'model_path': './results/MLP_902_2/', 'input_data_path': '../../knolling_dataset/MLP_unstack_902/labels_box/', 'output_data_path': '../../knolling_dataset/MLP_unstack_902/labels_unstack/', 'learning_rate': 0.0001, 'patience': 10, 'factor': 0.1, 'batch_size': 64, 'output_size': 6, 'abort_learning': 20, 'set_dropout': 0.1, 'num_boxes': 5, 'run_name': 'MLP_902_4', 'project_name': 'zzz_MLP_unstack', 'wandb_flag': True, 'use_mse': True, 'use_scaler': False, 'fine-tuning': False, 'node_1': 128, 'node_2': 32, 'node_3': 8}

 75%|███████▍  | 28765/38400 [00:03<00:01, 8226.92it/s]
total train data: 38400
100%|██████████| 38400/38400 [00:04<00:00, 7919.23it/s]
 74%|███████▍  | 7099/9600 [00:00<00:00, 8805.87it/s]
total valid data: 9600

100%|██████████| 9600/9600 [00:01<00:00, 8834.90it/s]
Training_Loss At Epoch 0:	 0.434566
Testing_Loss At Epoch 0:	 0.423031
epoch0, time used: 1.18, lr: 0.0001
Training_Loss At Epoch 1:	 0.389144
Testing_Loss At Epoch 1:	 0.36915
epoch1, time used: 0.8, lr: 0.0001
Training_Loss At Epoch 2:	 0.329234
Testing_Loss At Epoch 2:	 0.309903
epoch2, time used: 0.81, lr: 0.0001
Training_Loss At Epoch 3:	 0.290062
Testing_Loss At Epoch 3:	 0.289674
epoch3, time used: 0.76, lr: 0.0001
Training_Loss At Epoch 4:	 0.278587
Testing_Loss At Epoch 4:	 0.282672
epoch4, time used: 0.75, lr: 0.0001
Training_Loss At Epoch 5:	 0.270552
Testing_Loss At Epoch 5:	 0.271226
epoch5, time used: 0.81, lr: 0.0001
Training_Loss At Epoch 6:	 0.263426
Testing_Loss At Epoch 6:	 0.265433
epoch6, time used: 0.89, lr: 0.0001
Training_Loss At Epoch 7:	 0.257085
Testing_Loss At Epoch 7:	 0.257767
epoch7, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 8:	 0.251546
Testing_Loss At Epoch 8:	 0.254728
epoch8, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 9:	 0.24672
Testing_Loss At Epoch 9:	 0.24725
epoch9, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 10:	 0.242328
Testing_Loss At Epoch 10:	 0.242777
epoch10, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 11:	 0.238538
Testing_Loss At Epoch 11:	 0.240211
epoch11, time used: 0.79, lr: 0.0001
Training_Loss At Epoch 12:	 0.235163
Testing_Loss At Epoch 12:	 0.237802
epoch12, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 13:	 0.232086
Testing_Loss At Epoch 13:	 0.235245
epoch13, time used: 0.73, lr: 0.0001
Training_Loss At Epoch 14:	 0.22909
Testing_Loss At Epoch 14:	 0.230005
epoch14, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 15:	 0.226152
Testing_Loss At Epoch 15:	 0.227203
epoch15, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 16:	 0.223377
Testing_Loss At Epoch 16:	 0.225574
epoch16, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 17:	 0.220921
Testing_Loss At Epoch 17:	 0.221427
epoch17, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 18:	 0.218426
Testing_Loss At Epoch 18:	 0.220601
epoch18, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 19:	 0.215835
Testing_Loss At Epoch 19:	 0.216769
epoch19, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 20:	 0.213133
Testing_Loss At Epoch 20:	 0.21608
epoch20, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 21:	 0.210542
Testing_Loss At Epoch 21:	 0.211843
epoch21, time used: 0.79, lr: 0.0001
Training_Loss At Epoch 22:	 0.20806
Testing_Loss At Epoch 22:	 0.209438
epoch22, time used: 0.79, lr: 0.0001
Training_Loss At Epoch 23:	 0.204803
Testing_Loss At Epoch 23:	 0.20808
epoch23, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 24:	 0.202224
Testing_Loss At Epoch 24:	 0.205579
epoch24, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 25:	 0.19911
Testing_Loss At Epoch 25:	 0.204163
epoch25, time used: 0.77, lr: 0.0001
Training_Loss At Epoch 26:	 0.196896
Testing_Loss At Epoch 26:	 0.19899
epoch26, time used: 0.78, lr: 0.0001
epoch27, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 28:	 0.192019
Testing_Loss At Epoch 28:	 0.194665
epoch28, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 29:	 0.18951
Testing_Loss At Epoch 29:	 0.19349
epoch29, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 30:	 0.187678
Testing_Loss At Epoch 30:	 0.192565
epoch30, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 31:	 0.186233
Testing_Loss At Epoch 31:	 0.190434
epoch31, time used: 0.78, lr: 0.0001
epoch32, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 33:	 0.183586
Testing_Loss At Epoch 33:	 0.187804
epoch33, time used: 0.77, lr: 0.0001
epoch34, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 35:	 0.181376
Testing_Loss At Epoch 35:	 0.186901
epoch35, time used: 0.77, lr: 0.0001
Training_Loss At Epoch 36:	 0.180437
Testing_Loss At Epoch 36:	 0.186678
epoch36, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 37:	 0.180058
Testing_Loss At Epoch 37:	 0.184914
epoch37, time used: 0.74, lr: 0.0001
Training_Loss At Epoch 38:	 0.179166
Testing_Loss At Epoch 38:	 0.184569
epoch38, time used: 0.89, lr: 0.0001
epoch39, time used: 0.88, lr: 0.0001
epoch40, time used: 0.97, lr: 0.0001
epoch41, time used: 1.49, lr: 0.0001
Training_Loss At Epoch 42:	 0.176637
Testing_Loss At Epoch 42:	 0.183045
epoch42, time used: 0.82, lr: 0.0001
epoch43, time used: 0.82, lr: 0.0001
Training_Loss At Epoch 44:	 0.175863
Testing_Loss At Epoch 44:	 0.182856
epoch44, time used: 0.85, lr: 0.0001
epoch45, time used: 0.75, lr: 0.0001
epoch46, time used: 0.69, lr: 0.0001
epoch47, time used: 0.72, lr: 0.0001
Training_Loss At Epoch 48:	 0.174027
Testing_Loss At Epoch 48:	 0.182142
epoch48, time used: 0.68, lr: 0.0001
Training_Loss At Epoch 49:	 0.17425
Testing_Loss At Epoch 49:	 0.182103
epoch49, time used: 0.77, lr: 0.0001
Training_Loss At Epoch 50:	 0.173449
Testing_Loss At Epoch 50:	 0.181196
epoch50, time used: 0.83, lr: 0.0001
Training_Loss At Epoch 51:	 0.172974
Testing_Loss At Epoch 51:	 0.180158
epoch51, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 52:	 0.172805
Testing_Loss At Epoch 52:	 0.179626
epoch52, time used: 0.77, lr: 0.0001
epoch53, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 54:	 0.172076
Testing_Loss At Epoch 54:	 0.179509
epoch54, time used: 0.8, lr: 0.0001
Training_Loss At Epoch 55:	 0.171558
Testing_Loss At Epoch 55:	 0.179197
epoch55, time used: 0.79, lr: 0.0001
epoch56, time used: 0.78, lr: 0.0001
epoch57, time used: 0.74, lr: 0.0001
Training_Loss At Epoch 58:	 0.171166
Testing_Loss At Epoch 58:	 0.179152
epoch58, time used: 0.8, lr: 0.0001
epoch59, time used: 0.75, lr: 0.0001
Training_Loss At Epoch 60:	 0.170135
Testing_Loss At Epoch 60:	 0.177864
epoch60, time used: 0.8, lr: 0.0001
epoch61, time used: 0.81, lr: 0.0001
epoch62, time used: 0.75, lr: 0.0001
epoch63, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 64:	 0.169321
Testing_Loss At Epoch 64:	 0.177098
epoch64, time used: 0.77, lr: 0.0001
epoch65, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 66:	 0.168059
Testing_Loss At Epoch 66:	 0.176828
epoch66, time used: 0.76, lr: 0.0001
epoch67, time used: 0.68, lr: 0.0001
Training_Loss At Epoch 68:	 0.167987
Testing_Loss At Epoch 68:	 0.176204
epoch68, time used: 0.74, lr: 0.0001
epoch69, time used: 0.68, lr: 0.0001
epoch70, time used: 0.75, lr: 0.0001
Training_Loss At Epoch 71:	 0.167361
Testing_Loss At Epoch 71:	 0.17529
epoch71, time used: 0.75, lr: 0.0001
epoch72, time used: 0.81, lr: 0.0001
epoch73, time used: 0.81, lr: 0.0001
Training_Loss At Epoch 74:	 0.166134
Testing_Loss At Epoch 74:	 0.175267
epoch74, time used: 0.81, lr: 0.0001
Training_Loss At Epoch 75:	 0.165294
Testing_Loss At Epoch 75:	 0.175199
epoch75, time used: 0.71, lr: 0.0001
Training_Loss At Epoch 76:	 0.165585
Testing_Loss At Epoch 76:	 0.174928
epoch76, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 77:	 0.165345
Testing_Loss At Epoch 77:	 0.174022
epoch77, time used: 0.8, lr: 0.0001
epoch78, time used: 0.71, lr: 0.0001
Training_Loss At Epoch 79:	 0.164544
Testing_Loss At Epoch 79:	 0.173521
epoch79, time used: 0.77, lr: 0.0001
epoch80, time used: 0.78, lr: 0.0001
epoch81, time used: 0.77, lr: 0.0001
epoch82, time used: 0.76, lr: 0.0001
Training_Loss At Epoch 83:	 0.164052
Testing_Loss At Epoch 83:	 0.173145
epoch83, time used: 0.79, lr: 0.0001
epoch84, time used: 0.79, lr: 0.0001
epoch85, time used: 0.76, lr: 0.0001
epoch86, time used: 0.81, lr: 0.0001
Training_Loss At Epoch 87:	 0.162804
Testing_Loss At Epoch 87:	 0.17269
epoch87, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 88:	 0.162303
Testing_Loss At Epoch 88:	 0.171936
epoch88, time used: 0.71, lr: 0.0001
epoch89, time used: 0.75, lr: 0.0001
epoch90, time used: 0.8, lr: 0.0001
epoch91, time used: 0.86, lr: 0.0001
epoch92, time used: 0.81, lr: 0.0001
epoch93, time used: 0.77, lr: 0.0001
Training_Loss At Epoch 94:	 0.160919
Testing_Loss At Epoch 94:	 0.171887
epoch94, time used: 0.81, lr: 0.0001
Training_Loss At Epoch 95:	 0.160796
Testing_Loss At Epoch 95:	 0.171688
epoch95, time used: 1.12, lr: 0.0001
Training_Loss At Epoch 96:	 0.160579
Testing_Loss At Epoch 96:	 0.170501
epoch96, time used: 0.79, lr: 0.0001
epoch97, time used: 0.77, lr: 0.0001
Training_Loss At Epoch 98:	 0.159888
Testing_Loss At Epoch 98:	 0.170191
epoch98, time used: 1.08, lr: 0.0001
epoch99, time used: 1.47, lr: 0.0001
Training_Loss At Epoch 100:	 0.159639
Testing_Loss At Epoch 100:	 0.169483
epoch100, time used: 1.03, lr: 0.0001
epoch101, time used: 0.93, lr: 0.0001
epoch102, time used: 0.77, lr: 0.0001
epoch103, time used: 0.76, lr: 0.0001
epoch104, time used: 0.81, lr: 0.0001
epoch105, time used: 0.98, lr: 0.0001
Training_Loss At Epoch 106:	 0.158175
Testing_Loss At Epoch 106:	 0.168951
epoch106, time used: 0.77, lr: 0.0001
epoch107, time used: 0.8, lr: 0.0001
epoch108, time used: 0.77, lr: 0.0001
Training_Loss At Epoch 109:	 0.157824
Testing_Loss At Epoch 109:	 0.168813
epoch109, time used: 0.78, lr: 0.0001
epoch110, time used: 0.95, lr: 0.0001
Training_Loss At Epoch 111:	 0.157315
Testing_Loss At Epoch 111:	 0.168739
epoch111, time used: 0.97, lr: 0.0001
epoch112, time used: 0.8, lr: 0.0001
Training_Loss At Epoch 113:	 0.156605
Testing_Loss At Epoch 113:	 0.168355
epoch113, time used: 0.76, lr: 0.0001
epoch114, time used: 0.79, lr: 0.0001
Training_Loss At Epoch 115:	 0.156399
Testing_Loss At Epoch 115:	 0.168063
epoch115, time used: 0.79, lr: 0.0001
epoch116, time used: 0.75, lr: 0.0001
epoch117, time used: 0.79, lr: 0.0001
Training_Loss At Epoch 118:	 0.155519
Testing_Loss At Epoch 118:	 0.167576
epoch118, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 119:	 0.155654
Testing_Loss At Epoch 119:	 0.167109
epoch119, time used: 0.99, lr: 0.0001
epoch120, time used: 0.74, lr: 0.0001
Training_Loss At Epoch 121:	 0.155192
Testing_Loss At Epoch 121:	 0.167107
epoch121, time used: 0.73, lr: 0.0001
epoch122, time used: 0.81, lr: 0.0001
epoch123, time used: 0.73, lr: 0.0001
Training_Loss At Epoch 124:	 0.154475
Testing_Loss At Epoch 124:	 0.166609
epoch124, time used: 0.78, lr: 0.0001
epoch125, time used: 0.87, lr: 0.0001
epoch126, time used: 0.77, lr: 0.0001
epoch127, time used: 0.89, lr: 0.0001
epoch128, time used: 0.87, lr: 0.0001
epoch129, time used: 0.75, lr: 0.0001
Training_Loss At Epoch 130:	 0.153574
Testing_Loss At Epoch 130:	 0.165882
epoch130, time used: 0.79, lr: 0.0001
epoch131, time used: 0.79, lr: 0.0001
epoch132, time used: 0.72, lr: 0.0001
epoch133, time used: 0.73, lr: 0.0001
Training_Loss At Epoch 134:	 0.15336
Testing_Loss At Epoch 134:	 0.165806
epoch134, time used: 0.72, lr: 0.0001
Training_Loss At Epoch 135:	 0.153008
Testing_Loss At Epoch 135:	 0.165438
epoch135, time used: 0.76, lr: 0.0001
epoch136, time used: 0.8, lr: 0.0001
epoch137, time used: 0.79, lr: 0.0001
Training_Loss At Epoch 138:	 0.152476
Testing_Loss At Epoch 138:	 0.16526
epoch138, time used: 0.74, lr: 0.0001
epoch139, time used: 0.78, lr: 0.0001
epoch140, time used: 0.76, lr: 0.0001
epoch141, time used: 0.69, lr: 0.0001
Training_Loss At Epoch 142:	 0.151655
Testing_Loss At Epoch 142:	 0.165127
epoch142, time used: 0.83, lr: 0.0001
epoch143, time used: 0.88, lr: 0.0001
epoch144, time used: 0.86, lr: 0.0001
epoch145, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 146:	 0.151352
Testing_Loss At Epoch 146:	 0.164654
epoch146, time used: 0.83, lr: 0.0001
epoch147, time used: 0.74, lr: 0.0001
epoch148, time used: 0.78, lr: 0.0001
Training_Loss At Epoch 149:	 0.151048
Testing_Loss At Epoch 149:	 0.164597
epoch149, time used: 0.78, lr: 0.0001
epoch150, time used: 0.78, lr: 0.0001
epoch151, time used: 0.77, lr: 0.0001
epoch152, time used: 0.83, lr: 0.0001
Training_Loss At Epoch 153:	 0.150021
Testing_Loss At Epoch 153:	 0.164169
epoch153, time used: 0.79, lr: 0.0001
Training_Loss At Epoch 154:	 0.150494
Testing_Loss At Epoch 154:	 0.163614
epoch154, time used: 0.84, lr: 0.0001
epoch155, time used: 0.85, lr: 0.0001
epoch156, time used: 0.77, lr: 0.0001
Training_Loss At Epoch 157:	 0.149674
Testing_Loss At Epoch 157:	 0.16354
epoch157, time used: 0.81, lr: 0.0001
epoch158, time used: 0.79, lr: 0.0001
epoch159, time used: 0.73, lr: 0.0001
Training_Loss At Epoch 160:	 0.14929
Testing_Loss At Epoch 160:	 0.16353
epoch160, time used: 0.78, lr: 0.0001
epoch161, time used: 0.74, lr: 0.0001
Training_Loss At Epoch 162:	 0.14902
Testing_Loss At Epoch 162:	 0.163415
epoch162, time used: 0.73, lr: 0.0001
epoch163, time used: 0.79, lr: 0.0001
epoch164, time used: 0.78, lr: 0.0001
epoch165, time used: 0.76, lr: 0.0001
epoch166, time used: 0.77, lr: 0.0001
Training_Loss At Epoch 167:	 0.148403
Testing_Loss At Epoch 167:	 0.163296
epoch167, time used: 0.78, lr: 0.0001
epoch168, time used: 0.77, lr: 0.0001
Training_Loss At Epoch 169:	 0.148043
Testing_Loss At Epoch 169:	 0.163022
epoch169, time used: 0.69, lr: 0.0001
epoch170, time used: 0.71, lr: 0.0001
Training_Loss At Epoch 171:	 0.148099
Testing_Loss At Epoch 171:	 0.162931
epoch171, time used: 0.68, lr: 0.0001
Training_Loss At Epoch 172:	 0.148045
Testing_Loss At Epoch 172:	 0.162575
epoch172, time used: 0.69, lr: 0.0001
epoch173, time used: 0.75, lr: 0.0001
epoch174, time used: 0.79, lr: 0.0001
epoch175, time used: 0.67, lr: 0.0001
epoch176, time used: 0.86, lr: 0.0001
epoch177, time used: 0.83, lr: 0.0001
epoch178, time used: 0.8, lr: 0.0001
epoch179, time used: 0.87, lr: 0.0001
epoch180, time used: 0.77, lr: 0.0001
epoch181, time used: 0.77, lr: 0.0001
Training_Loss At Epoch 182:	 0.146952
Testing_Loss At Epoch 182:	 0.162298
epoch182, time used: 0.78, lr: 0.0001
epoch183, time used: 0.81, lr: 0.0001
epoch184, time used: 0.89, lr: 0.0001
epoch185, time used: 0.81, lr: 0.0001
Training_Loss At Epoch 186:	 0.146429
Testing_Loss At Epoch 186:	 0.161966
epoch186, time used: 0.88, lr: 0.0001
epoch187, time used: 0.87, lr: 0.0001
epoch188, time used: 0.77, lr: 0.0001
epoch189, time used: 0.79, lr: 0.0001
epoch190, time used: 0.72, lr: 0.0001
epoch191, time used: 0.79, lr: 0.0001
epoch192, time used: 0.72, lr: 0.0001
Training_Loss At Epoch 193:	 0.146025
Testing_Loss At Epoch 193:	 0.161338
epoch193, time used: 0.78, lr: 0.0001
epoch194, time used: 0.91, lr: 0.0001
epoch195, time used: 0.82, lr: 0.0001
epoch196, time used: 0.79, lr: 0.0001
epoch197, time used: 0.77, lr: 0.0001
epoch198, time used: 0.79, lr: 0.0001
epoch199, time used: 0.77, lr: 0.0001
Training_Loss At Epoch 200:	 0.145058
Testing_Loss At Epoch 200:	 0.160857
epoch200, time used: 0.79, lr: 0.0001
epoch201, time used: 0.74, lr: 0.0001
epoch202, time used: 1.35, lr: 0.0001
epoch203, time used: 1.18, lr: 0.0001
epoch204, time used: 0.77, lr: 0.0001
Training_Loss At Epoch 205:	 0.144445
Testing_Loss At Epoch 205:	 0.1607
epoch205, time used: 0.81, lr: 0.0001
epoch206, time used: 1.05, lr: 0.0001
Training_Loss At Epoch 207:	 0.14463
Testing_Loss At Epoch 207:	 0.160558
epoch207, time used: 1.02, lr: 0.0001
epoch208, time used: 0.93, lr: 0.0001
epoch209, time used: 0.89, lr: 0.0001
epoch210, time used: 0.72, lr: 0.0001
epoch211, time used: 0.78, lr: 0.0001
epoch212, time used: 0.78, lr: 0.0001
epoch213, time used: 0.79, lr: 0.0001
Training_Loss At Epoch 214:	 0.14339
Testing_Loss At Epoch 214:	 0.160391
epoch214, time used: 0.71, lr: 0.0001
Training_Loss At Epoch 215:	 0.143476
Testing_Loss At Epoch 215:	 0.159967
epoch215, time used: 0.92, lr: 0.0001
Training_Loss At Epoch 216:	 0.143314
Testing_Loss At Epoch 216:	 0.159383
epoch216, time used: 0.79, lr: 0.0001
epoch217, time used: 0.73, lr: 0.0001
epoch218, time used: 0.81, lr: 0.0001
epoch219, time used: 0.8, lr: 0.0001
epoch220, time used: 0.8, lr: 0.0001
epoch221, time used: 0.88, lr: 0.0001
epoch222, time used: 0.77, lr: 0.0001
epoch223, time used: 0.77, lr: 0.0001
epoch224, time used: 0.93, lr: 0.0001
epoch225, time used: 0.81, lr: 0.0001
epoch226, time used: 1.05, lr: 0.0001
epoch227, time used: 0.7, lr: 0.0001
Training_Loss At Epoch 228:	 0.14012
Testing_Loss At Epoch 228:	 0.158874
epoch228, time used: 0.76, lr: 1e-05
Training_Loss At Epoch 229:	 0.139833
Testing_Loss At Epoch 229:	 0.158508
epoch229, time used: 1.32, lr: 1e-05
epoch230, time used: 0.71, lr: 1e-05
epoch231, time used: 0.69, lr: 1e-05
Training_Loss At Epoch 232:	 0.139713
Testing_Loss At Epoch 232:	 0.158434
epoch232, time used: 0.83, lr: 1e-05
Training_Loss At Epoch 233:	 0.139671
Testing_Loss At Epoch 233:	 0.158393
epoch233, time used: 1.02, lr: 1e-05
epoch234, time used: 0.8, lr: 1e-05
epoch235, time used: 0.78, lr: 1e-05
epoch236, time used: 0.84, lr: 1e-05
epoch237, time used: 1.4, lr: 1e-05
epoch238, time used: 0.7, lr: 1e-05
epoch239, time used: 0.7, lr: 1e-05
epoch240, time used: 0.7, lr: 1e-05
epoch241, time used: 0.79, lr: 1e-05
Training_Loss At Epoch 242:	 0.13958
Testing_Loss At Epoch 242:	 0.158364
epoch242, time used: 0.75, lr: 1e-05
epoch243, time used: 0.94, lr: 1e-05
epoch244, time used: 0.83, lr: 1e-05
epoch245, time used: 0.79, lr: 1e-05
epoch246, time used: 0.84, lr: 1e-05
epoch247, time used: 0.82, lr: 1e-05
epoch248, time used: 0.78, lr: 1e-05
epoch249, time used: 0.94, lr: 1e-05
epoch250, time used: 1.0, lr: 1e-05
epoch251, time used: 0.78, lr: 1e-05
epoch252, time used: 0.72, lr: 1e-05
epoch253, time used: 0.8, lr: 1e-05
epoch254, time used: 0.73, lr: 1.0000000000000002e-06
epoch255, time used: 0.93, lr: 1.0000000000000002e-06
epoch256, time used: 1.05, lr: 1.0000000000000002e-06
epoch257, time used: 0.82, lr: 1.0000000000000002e-06
epoch258, time used: 0.84, lr: 1.0000000000000002e-06
epoch259, time used: 0.96, lr: 1.0000000000000002e-06
epoch260, time used: 0.77, lr: 1.0000000000000002e-06
epoch261, time used: 0.78, lr: 1.0000000000000002e-06
epoch262, time used: 0.98, lr: 1.0000000000000002e-06
epoch263, time used: 1.0, lr: 1.0000000000000002e-06