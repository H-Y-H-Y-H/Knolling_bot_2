100%|██████████| 3840/3840 [00:00<00:00, 9011.11it/s]
100%|██████████| 960/960 [00:00<00:00, 9014.96it/s]
this is para_dict
 {'device': 'cuda:0', 'num_img': 4800, 'ratio': 0.8, 'epoch': 300, 'model_path': './results/MLP_902_1/', 'input_data_path': '../../knolling_dataset/MLP_unstack_902/labels_box/', 'output_data_path': '../../knolling_dataset/MLP_unstack_902/labels_unstack/', 'learning_rate': 0.001, 'patience': 10, 'factor': 0.1, 'batch_size': 64, 'output_size': 6, 'abort_learning': 20, 'set_dropout': 0.1, 'num_boxes': 5, 'run_name': 'MLP_902_1', 'project_name': 'zzz_MLP_unstack', 'wandb_flag': True, 'use_mse': True, 'use_scaler': False, 'fine-tuning': False, 'node_1': 128, 'node_2': 32, 'node_3': 8}
load the train data ...
total train data: 3840
load the valid data ...
total valid data: 960
not fine-tuning
Training_Loss At Epoch 0:	 0.396145
Testing_Loss At Epoch 0:	 0.371902
epoch0, time used: 0.56, lr: 0.001
Training_Loss At Epoch 1:	 0.357779
Testing_Loss At Epoch 1:	 0.359639
epoch1, time used: 0.09, lr: 0.001
Training_Loss At Epoch 2:	 0.3449
Testing_Loss At Epoch 2:	 0.340582
epoch2, time used: 0.09, lr: 0.001
Training_Loss At Epoch 3:	 0.314299
Testing_Loss At Epoch 3:	 0.306549
epoch3, time used: 0.08, lr: 0.001
Training_Loss At Epoch 4:	 0.289988
Testing_Loss At Epoch 4:	 0.284498
epoch4, time used: 0.08, lr: 0.001
Training_Loss At Epoch 5:	 0.277504
Testing_Loss At Epoch 5:	 0.273818
epoch5, time used: 0.09, lr: 0.001
Training_Loss At Epoch 6:	 0.267265
Testing_Loss At Epoch 6:	 0.26539
epoch6, time used: 0.12, lr: 0.001
Training_Loss At Epoch 7:	 0.258924
Testing_Loss At Epoch 7:	 0.260502
epoch7, time used: 0.08, lr: 0.001
Training_Loss At Epoch 8:	 0.253331
Testing_Loss At Epoch 8:	 0.258145
epoch8, time used: 0.08, lr: 0.001
epoch9, time used: 0.13, lr: 0.001
Training_Loss At Epoch 10:	 0.245596
Testing_Loss At Epoch 10:	 0.248985
epoch10, time used: 0.08, lr: 0.001
Training_Loss At Epoch 11:	 0.241194
Testing_Loss At Epoch 11:	 0.246739
epoch11, time used: 0.08, lr: 0.001
epoch12, time used: 0.11, lr: 0.001
Training_Loss At Epoch 13:	 0.232096
Testing_Loss At Epoch 13:	 0.242989
epoch13, time used: 0.08, lr: 0.001
epoch14, time used: 0.08, lr: 0.001
Training_Loss At Epoch 15:	 0.229035
Testing_Loss At Epoch 15:	 0.240456
epoch15, time used: 0.1, lr: 0.001
Training_Loss At Epoch 16:	 0.223381
Testing_Loss At Epoch 16:	 0.239298
epoch16, time used: 0.08, lr: 0.001
Training_Loss At Epoch 17:	 0.220127
Testing_Loss At Epoch 17:	 0.236891
epoch17, time used: 0.08, lr: 0.001
epoch18, time used: 0.13, lr: 0.001
Training_Loss At Epoch 19:	 0.212365
Testing_Loss At Epoch 19:	 0.229104
epoch19, time used: 0.09, lr: 0.001
epoch20, time used: 0.08, lr: 0.001
epoch21, time used: 0.1, lr: 0.001
Training_Loss At Epoch 22:	 0.207335
Testing_Loss At Epoch 22:	 0.228357
epoch22, time used: 0.08, lr: 0.001
Training_Loss At Epoch 23:	 0.203563
Testing_Loss At Epoch 23:	 0.226985
epoch23, time used: 0.08, lr: 0.001
Training_Loss At Epoch 24:	 0.198672
Testing_Loss At Epoch 24:	 0.224757
epoch24, time used: 0.12, lr: 0.001
epoch25, time used: 0.08, lr: 0.001
Training_Loss At Epoch 26:	 0.193548
Testing_Loss At Epoch 26:	 0.223977
epoch26, time used: 0.08, lr: 0.001
epoch27, time used: 0.12, lr: 0.001
Training_Loss At Epoch 28:	 0.188414
Testing_Loss At Epoch 28:	 0.218746
epoch28, time used: 0.08, lr: 0.001
epoch29, time used: 0.08, lr: 0.001
epoch30, time used: 0.1, lr: 0.001
epoch31, time used: 0.07, lr: 0.001
Training_Loss At Epoch 32:	 0.181676
Testing_Loss At Epoch 32:	 0.217897
epoch32, time used: 0.07, lr: 0.001
epoch33, time used: 0.1, lr: 0.001
epoch34, time used: 0.08, lr: 0.001
epoch35, time used: 0.08, lr: 0.001
Training_Loss At Epoch 36:	 0.174798
Testing_Loss At Epoch 36:	 0.213961
epoch36, time used: 0.1, lr: 0.001
epoch37, time used: 0.08, lr: 0.001
epoch38, time used: 0.08, lr: 0.001
epoch39, time used: 0.11, lr: 0.001
epoch40, time used: 0.08, lr: 0.001
epoch41, time used: 0.08, lr: 0.001
Training_Loss At Epoch 42:	 0.169017
Testing_Loss At Epoch 42:	 0.21352
epoch42, time used: 0.09, lr: 0.001
epoch43, time used: 0.08, lr: 0.001
Training_Loss At Epoch 44:	 0.162805
Testing_Loss At Epoch 44:	 0.207977
epoch44, time used: 0.08, lr: 0.001
epoch45, time used: 0.11, lr: 0.001
epoch46, time used: 0.08, lr: 0.001
epoch47, time used: 0.08, lr: 0.001
epoch48, time used: 0.08, lr: 0.001
epoch49, time used: 0.08, lr: 0.001
epoch50, time used: 0.08, lr: 0.001
epoch51, time used: 0.09, lr: 0.001
epoch52, time used: 0.07, lr: 0.001
epoch53, time used: 0.08, lr: 0.001
epoch54, time used: 0.09, lr: 0.001
epoch55, time used: 0.07, lr: 0.001
epoch56, time used: 0.08, lr: 0.0001
epoch57, time used: 0.09, lr: 0.0001
epoch58, time used: 0.08, lr: 0.0001
epoch59, time used: 0.07, lr: 0.0001
epoch60, time used: 0.09, lr: 0.0001
epoch61, time used: 0.08, lr: 0.0001
epoch62, time used: 0.08, lr: 0.0001
epoch63, time used: 0.09, lr: 0.0001
epoch64, time used: 0.07, lr: 0.0001
epoch65, time used: 0.08, lr: 0.0001