this is para_dict
 {'device': 'cuda:0', 'num_img': 48000, 'ratio': 0.8, 'epoch': 300, 'model_path': './results/MLP_902_1/', 'input_data_path': '../../knolling_dataset/MLP_unstack_902/labels_box/', 'output_data_path': '../../knolling_dataset/MLP_unstack_902/labels_unstack/', 'learning_rate': 0.001, 'patience': 10, 'factor': 0.1, 'batch_size': 64, 'output_size': 6, 'abort_learning': 20, 'set_dropout': 0.1, 'num_boxes': 5, 'run_name': 'MLP_902_1', 'project_name': 'zzz_MLP_unstack', 'wandb_flag': True, 'use_mse': True, 'use_scaler': False, 'fine-tuning': False, 'node_1': 128, 'node_2': 32, 'node_3': 8}
load the train data ...

100%|██████████| 38400/38400 [00:04<00:00, 8790.63it/s]
100%|██████████| 9600/9600 [00:01<00:00, 8879.75it/s]
total train data: 38400
load the valid data ...
total valid data: 9600
not fine-tuning
Training_Loss At Epoch 0:	 0.303109
Testing_Loss At Epoch 0:	 0.251286
epoch0, time used: 1.26, lr: 0.001
Training_Loss At Epoch 1:	 0.230113
Testing_Loss At Epoch 1:	 0.220351
epoch1, time used: 0.78, lr: 0.001
Training_Loss At Epoch 2:	 0.212938
Testing_Loss At Epoch 2:	 0.203687
epoch2, time used: 0.71, lr: 0.001
Training_Loss At Epoch 3:	 0.19681
Testing_Loss At Epoch 3:	 0.197808
epoch3, time used: 0.67, lr: 0.001
Training_Loss At Epoch 4:	 0.185823
Testing_Loss At Epoch 4:	 0.180787
epoch4, time used: 0.7, lr: 0.001
epoch5, time used: 0.76, lr: 0.001
Training_Loss At Epoch 6:	 0.174804
Testing_Loss At Epoch 6:	 0.175604
epoch6, time used: 0.73, lr: 0.001
Training_Loss At Epoch 7:	 0.171383
Testing_Loss At Epoch 7:	 0.173889
epoch7, time used: 0.79, lr: 0.001
epoch8, time used: 0.79, lr: 0.001
Training_Loss At Epoch 9:	 0.166012
Testing_Loss At Epoch 9:	 0.171721
epoch9, time used: 0.72, lr: 0.001
epoch10, time used: 0.72, lr: 0.001
Training_Loss At Epoch 11:	 0.162285
Testing_Loss At Epoch 11:	 0.170402
epoch11, time used: 0.78, lr: 0.001
epoch12, time used: 0.78, lr: 0.001
epoch13, time used: 0.77, lr: 0.001
Training_Loss At Epoch 14:	 0.157874
Testing_Loss At Epoch 14:	 0.161686
epoch14, time used: 0.77, lr: 0.001
Training_Loss At Epoch 15:	 0.156878
Testing_Loss At Epoch 15:	 0.160991
epoch15, time used: 0.78, lr: 0.001
epoch16, time used: 0.78, lr: 0.001
Training_Loss At Epoch 17:	 0.154999
Testing_Loss At Epoch 17:	 0.160031
epoch17, time used: 0.72, lr: 0.001
epoch18, time used: 1.0, lr: 0.001
epoch19, time used: 0.77, lr: 0.001
epoch20, time used: 0.85, lr: 0.001
Training_Loss At Epoch 21:	 0.150564
Testing_Loss At Epoch 21:	 0.158869
epoch21, time used: 0.78, lr: 0.001
epoch22, time used: 0.78, lr: 0.001
Training_Loss At Epoch 23:	 0.149019
Testing_Loss At Epoch 23:	 0.15728
epoch23, time used: 0.78, lr: 0.001
epoch24, time used: 0.79, lr: 0.001
epoch25, time used: 0.74, lr: 0.001
epoch26, time used: 0.87, lr: 0.001
epoch27, time used: 0.79, lr: 0.001
epoch28, time used: 0.74, lr: 0.001
Training_Loss At Epoch 29:	 0.145174
Testing_Loss At Epoch 29:	 0.156083
epoch29, time used: 1.01, lr: 0.001
Training_Loss At Epoch 30:	 0.14397
Testing_Loss At Epoch 30:	 0.153066
epoch30, time used: 0.8, lr: 0.001
epoch31, time used: 0.78, lr: 0.001
epoch32, time used: 0.75, lr: 0.001
Training_Loss At Epoch 33:	 0.141333
Testing_Loss At Epoch 33:	 0.152146
epoch33, time used: 0.79, lr: 0.001
epoch34, time used: 0.81, lr: 0.001
epoch35, time used: 0.79, lr: 0.001
epoch36, time used: 0.73, lr: 0.001
Training_Loss At Epoch 37:	 0.139423
Testing_Loss At Epoch 37:	 0.150621
epoch37, time used: 0.8, lr: 0.001
epoch38, time used: 0.76, lr: 0.001
epoch39, time used: 0.73, lr: 0.001
epoch40, time used: 0.79, lr: 0.001
Training_Loss At Epoch 41:	 0.137897
Testing_Loss At Epoch 41:	 0.149943
epoch41, time used: 0.76, lr: 0.001
epoch42, time used: 0.79, lr: 0.001
epoch43, time used: 0.77, lr: 0.001
Training_Loss At Epoch 44:	 0.135985
Testing_Loss At Epoch 44:	 0.14969
epoch44, time used: 0.79, lr: 0.001
epoch45, time used: 0.78, lr: 0.001
epoch46, time used: 0.76, lr: 0.001
epoch47, time used: 0.74, lr: 0.001
epoch48, time used: 0.82, lr: 0.001
Training_Loss At Epoch 49:	 0.133063
Testing_Loss At Epoch 49:	 0.148446
epoch49, time used: 0.8, lr: 0.001
epoch50, time used: 0.81, lr: 0.001
epoch51, time used: 1.0, lr: 0.001
epoch52, time used: 0.77, lr: 0.001
epoch53, time used: 0.89, lr: 0.001
epoch54, time used: 0.77, lr: 0.001
epoch55, time used: 0.77, lr: 0.001
epoch56, time used: 0.82, lr: 0.001
epoch57, time used: 0.84, lr: 0.001
epoch58, time used: 0.81, lr: 0.001
epoch59, time used: 0.95, lr: 0.001
epoch60, time used: 0.78, lr: 0.001
Training_Loss At Epoch 61:	 0.120518
Testing_Loss At Epoch 61:	 0.146015
epoch61, time used: 0.72, lr: 0.0001
Training_Loss At Epoch 62:	 0.118553
Testing_Loss At Epoch 62:	 0.145839
epoch62, time used: 0.81, lr: 0.0001
epoch63, time used: 0.82, lr: 0.0001
Training_Loss At Epoch 64:	 0.117906
Testing_Loss At Epoch 64:	 0.145794
epoch64, time used: 0.79, lr: 0.0001
Training_Loss At Epoch 65:	 0.117601
Testing_Loss At Epoch 65:	 0.145396
epoch65, time used: 0.79, lr: 0.0001
epoch66, time used: 0.81, lr: 0.0001
epoch67, time used: 0.78, lr: 0.0001
epoch68, time used: 0.76, lr: 0.0001
epoch69, time used: 0.77, lr: 0.0001
epoch70, time used: 0.79, lr: 0.0001
epoch71, time used: 0.79, lr: 0.0001
epoch72, time used: 0.72, lr: 0.0001
epoch73, time used: 0.81, lr: 0.0001
epoch74, time used: 0.77, lr: 0.0001
epoch75, time used: 0.78, lr: 0.0001
epoch76, time used: 0.72, lr: 0.0001
Training_Loss At Epoch 77:	 0.114524
Testing_Loss At Epoch 77:	 0.145366
epoch77, time used: 0.69, lr: 1e-05
Training_Loss At Epoch 78:	 0.114376
Testing_Loss At Epoch 78:	 0.145346
epoch78, time used: 0.7, lr: 1e-05
epoch79, time used: 0.77, lr: 1e-05
epoch80, time used: 0.78, lr: 1e-05
epoch81, time used: 0.81, lr: 1e-05
Training_Loss At Epoch 82:	 0.114246
Testing_Loss At Epoch 82:	 0.145325
epoch82, time used: 0.78, lr: 1e-05
Training_Loss At Epoch 83:	 0.114217
Testing_Loss At Epoch 83:	 0.145321
epoch83, time used: 0.8, lr: 1e-05
epoch84, time used: 0.81, lr: 1e-05
Training_Loss At Epoch 85:	 0.114197
Testing_Loss At Epoch 85:	 0.145311
epoch85, time used: 0.77, lr: 1e-05
epoch86, time used: 0.8, lr: 1e-05
epoch87, time used: 0.8, lr: 1e-05
epoch88, time used: 0.8, lr: 1e-05
epoch89, time used: 0.8, lr: 1e-05
epoch90, time used: 0.79, lr: 1e-05
epoch91, time used: 0.78, lr: 1e-05
epoch92, time used: 0.77, lr: 1e-05
epoch93, time used: 0.69, lr: 1e-05
epoch94, time used: 0.69, lr: 1.0000000000000002e-06
epoch95, time used: 0.69, lr: 1.0000000000000002e-06
epoch96, time used: 0.77, lr: 1.0000000000000002e-06
epoch97, time used: 0.75, lr: 1.0000000000000002e-06
epoch98, time used: 0.79, lr: 1.0000000000000002e-06
epoch99, time used: 0.81, lr: 1.0000000000000002e-06
epoch100, time used: 0.77, lr: 1.0000000000000002e-06
epoch101, time used: 0.76, lr: 1.0000000000000002e-06
epoch102, time used: 0.78, lr: 1.0000000000000002e-06
epoch103, time used: 0.79, lr: 1.0000000000000002e-06
epoch104, time used: 0.78, lr: 1.0000000000000002e-06
epoch105, time used: 0.79, lr: 1.0000000000000002e-07
epoch106, time used: 0.78, lr: 1.0000000000000002e-07