this is para_dict
 {'device': 'cuda:0', 'num_img': 48000, 'ratio': 0.8, 'epoch': 300, 'model_path': './results/MLP_902_2/', 'input_data_path': '../../knolling_dataset/MLP_unstack_902/labels_box/', 'output_data_path': '../../knolling_dataset/MLP_unstack_902/labels_unstack/', 'learning_rate': 0.001, 'patience': 10, 'factor': 0.1, 'batch_size': 64, 'output_size': 6, 'abort_learning': 20, 'set_dropout': 0.1, 'num_boxes': 5, 'project_name': 'zzz_MLP_unstack', 'wandb_flag': True, 'use_mse': True, 'use_scaler': False, 'fine-tuning': False, 'node_1': 128, 'node_2': 32, 'node_3': 8}
load the train data ...

100%|██████████| 38400/38400 [00:04<00:00, 8934.48it/s]
100%|██████████| 9600/9600 [00:01<00:00, 9087.89it/s]
total train data: 38400
load the valid data ...
total valid data: 9600
not fine-tuning
Training_Loss At Epoch 0:	 0.29551
Testing_Loss At Epoch 0:	 0.243757
epoch0, time used: 1.24, lr: 0.001
Training_Loss At Epoch 1:	 0.230925
Testing_Loss At Epoch 1:	 0.221777
epoch1, time used: 1.02, lr: 0.001
Training_Loss At Epoch 2:	 0.213129
Testing_Loss At Epoch 2:	 0.203813
epoch2, time used: 0.89, lr: 0.001
Training_Loss At Epoch 3:	 0.195862
Testing_Loss At Epoch 3:	 0.18836
epoch3, time used: 0.89, lr: 0.001
Training_Loss At Epoch 4:	 0.182951
Testing_Loss At Epoch 4:	 0.179791
epoch4, time used: 0.77, lr: 0.001
Training_Loss At Epoch 5:	 0.175779
Testing_Loss At Epoch 5:	 0.178304
epoch5, time used: 0.81, lr: 0.001
Training_Loss At Epoch 6:	 0.171996
Testing_Loss At Epoch 6:	 0.175081
epoch6, time used: 0.84, lr: 0.001
epoch7, time used: 0.8, lr: 0.001
Training_Loss At Epoch 8:	 0.165497
Testing_Loss At Epoch 8:	 0.172809
epoch8, time used: 0.86, lr: 0.001
Training_Loss At Epoch 9:	 0.163665
Testing_Loss At Epoch 9:	 0.170098
epoch9, time used: 0.79, lr: 0.001
Training_Loss At Epoch 10:	 0.16071
Testing_Loss At Epoch 10:	 0.164367
epoch10, time used: 0.92, lr: 0.001
Training_Loss At Epoch 11:	 0.158084
Testing_Loss At Epoch 11:	 0.164272
epoch11, time used: 0.83, lr: 0.001
Training_Loss At Epoch 12:	 0.15822
Testing_Loss At Epoch 12:	 0.164199
epoch12, time used: 0.82, lr: 0.001
Training_Loss At Epoch 13:	 0.156084
Testing_Loss At Epoch 13:	 0.164177
epoch13, time used: 0.89, lr: 0.001
Training_Loss At Epoch 14:	 0.155416
Testing_Loss At Epoch 14:	 0.161812
epoch14, time used: 0.73, lr: 0.001
epoch15, time used: 0.88, lr: 0.001
epoch16, time used: 0.88, lr: 0.001
epoch17, time used: 0.79, lr: 0.001
epoch18, time used: 0.83, lr: 0.001
epoch19, time used: 0.8, lr: 0.001
Training_Loss At Epoch 20:	 0.151385
Testing_Loss At Epoch 20:	 0.158167
epoch20, time used: 0.88, lr: 0.001
epoch21, time used: 0.77, lr: 0.001
Training_Loss At Epoch 22:	 0.148502
Testing_Loss At Epoch 22:	 0.155604
epoch22, time used: 0.7, lr: 0.001
epoch23, time used: 0.76, lr: 0.001
epoch24, time used: 0.77, lr: 0.001
epoch25, time used: 0.77, lr: 0.001
epoch26, time used: 0.7, lr: 0.001
epoch27, time used: 0.77, lr: 0.001
epoch28, time used: 0.78, lr: 0.001
epoch29, time used: 0.78, lr: 0.001
epoch30, time used: 0.8, lr: 0.001
epoch31, time used: 0.8, lr: 0.001
epoch32, time used: 0.79, lr: 0.001
Training_Loss At Epoch 33:	 0.142096
Testing_Loss At Epoch 33:	 0.152567
epoch33, time used: 0.78, lr: 0.001
epoch34, time used: 0.74, lr: 0.001
epoch35, time used: 0.78, lr: 0.001
epoch36, time used: 0.78, lr: 0.001
epoch37, time used: 1.2, lr: 0.001
Training_Loss At Epoch 38:	 0.138544
Testing_Loss At Epoch 38:	 0.151765
epoch38, time used: 0.77, lr: 0.001
epoch39, time used: 0.78, lr: 0.001
epoch40, time used: 0.76, lr: 0.001
Training_Loss At Epoch 41:	 0.137457
Testing_Loss At Epoch 41:	 0.151639
epoch41, time used: 0.7, lr: 0.001
epoch42, time used: 0.78, lr: 0.001
epoch43, time used: 0.81, lr: 0.001
epoch44, time used: 0.83, lr: 0.001
epoch45, time used: 0.77, lr: 0.001
epoch46, time used: 0.8, lr: 0.001
epoch47, time used: 0.75, lr: 0.001
epoch48, time used: 0.77, lr: 0.001
epoch49, time used: 0.81, lr: 0.001
epoch50, time used: 0.77, lr: 0.001
Training_Loss At Epoch 51:	 0.13131
Testing_Loss At Epoch 51:	 0.150059
epoch51, time used: 0.77, lr: 0.001
epoch52, time used: 0.79, lr: 0.001
epoch53, time used: 0.77, lr: 0.001
epoch54, time used: 0.79, lr: 0.001
epoch55, time used: 0.81, lr: 0.001
epoch56, time used: 0.79, lr: 0.001
epoch57, time used: 0.8, lr: 0.001
epoch58, time used: 0.79, lr: 0.001
Training_Loss At Epoch 59:	 0.128551
Testing_Loss At Epoch 59:	 0.148838
epoch59, time used: 1.22, lr: 0.001
epoch60, time used: 0.79, lr: 0.001
Training_Loss At Epoch 61:	 0.127599
Testing_Loss At Epoch 61:	 0.14815
epoch61, time used: 0.78, lr: 0.001
epoch62, time used: 0.79, lr: 0.001
epoch63, time used: 0.78, lr: 0.001
epoch64, time used: 0.77, lr: 0.001
epoch65, time used: 0.77, lr: 0.001
epoch66, time used: 0.79, lr: 0.001
epoch67, time used: 0.8, lr: 0.001
epoch68, time used: 0.9, lr: 0.001
epoch69, time used: 0.78, lr: 0.001
epoch70, time used: 0.85, lr: 0.001
epoch71, time used: 0.8, lr: 0.001
epoch72, time used: 0.78, lr: 0.001
Training_Loss At Epoch 73:	 0.113571
Testing_Loss At Epoch 73:	 0.147653
epoch73, time used: 0.77, lr: 0.0001
Training_Loss At Epoch 74:	 0.111825
Testing_Loss At Epoch 74:	 0.147036
epoch74, time used: 0.71, lr: 0.0001
epoch75, time used: 0.77, lr: 0.0001
Training_Loss At Epoch 76:	 0.111205
Testing_Loss At Epoch 76:	 0.146747
epoch76, time used: 0.77, lr: 0.0001
epoch77, time used: 0.8, lr: 0.0001
epoch78, time used: 0.79, lr: 0.0001
epoch79, time used: 0.77, lr: 0.0001
epoch80, time used: 0.8, lr: 0.0001
epoch81, time used: 0.78, lr: 0.0001
epoch82, time used: 0.8, lr: 0.0001
epoch83, time used: 0.76, lr: 0.0001
epoch84, time used: 0.76, lr: 0.0001
epoch85, time used: 0.74, lr: 0.0001
epoch86, time used: 0.79, lr: 0.0001
epoch87, time used: 0.77, lr: 0.0001
epoch88, time used: 0.79, lr: 1e-05
epoch89, time used: 0.77, lr: 1e-05
epoch90, time used: 0.77, lr: 1e-05
epoch91, time used: 0.79, lr: 1e-05
epoch92, time used: 0.73, lr: 1e-05
epoch93, time used: 0.76, lr: 1e-05
epoch94, time used: 0.76, lr: 1e-05
epoch95, time used: 0.77, lr: 1e-05
epoch96, time used: 0.78, lr: 1e-05
epoch97, time used: 0.74, lr: 1e-05