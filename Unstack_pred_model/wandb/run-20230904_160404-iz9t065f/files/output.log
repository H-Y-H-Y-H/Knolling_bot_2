
 38%|███▊      | 14509/38400 [00:01<00:02, 8975.19it/s]
this is para_dict
 {'device': 'cuda:0', 'num_img': 48000, 'ratio': 0.8, 'epoch': 300, 'model_path': './results/MLP_902_2/', 'input_data_path': '../../knolling_dataset/MLP_unstack_902/labels_box/', 'output_data_path': '../../knolling_dataset/MLP_unstack_902/labels_unstack/', 'learning_rate': 0.001, 'patience': 10, 'factor': 0.1, 'batch_size': 64, 'output_size': 6, 'abort_learning': 20, 'set_dropout': 0.1, 'num_boxes': 5, 'run_name': 'MLP_902_3', 'project_name': 'zzz_MLP_unstack', 'wandb_flag': True, 'use_mse': True, 'use_scaler': False, 'fine-tuning': False, 'node_1': 128, 'node_2': 32, 'node_3': 8}

100%|██████████| 38400/38400 [00:04<00:00, 8831.90it/s]
100%|██████████| 9600/9600 [00:01<00:00, 8250.62it/s]
total train data: 38400
load the valid data ...
total valid data: 9600
not fine-tuning
Training_Loss At Epoch 0:	 0.2963
Testing_Loss At Epoch 0:	 0.237856
epoch0, time used: 1.32, lr: 0.001
Training_Loss At Epoch 1:	 0.228092
Testing_Loss At Epoch 1:	 0.216803
epoch1, time used: 0.98, lr: 0.001
Training_Loss At Epoch 2:	 0.211898
Testing_Loss At Epoch 2:	 0.203622
