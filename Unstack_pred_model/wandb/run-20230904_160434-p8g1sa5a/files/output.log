this is para_dict
 {'device': 'cuda:0', 'num_img': 48000, 'ratio': 0.8, 'epoch': 300, 'model_path': './results/MLP_902_2/', 'input_data_path': '../../knolling_dataset/MLP_unstack_902/labels_box/', 'output_data_path': '../../knolling_dataset/MLP_unstack_902/labels_unstack/', 'learning_rate': 0.001, 'patience': 10, 'factor': 0.1, 'batch_size': 64, 'output_size': 6, 'abort_learning': 20, 'set_dropout': 0.1, 'num_boxes': 5, 'run_name': 'MLP_902_3', 'project_name': 'zzz_MLP_unstack', 'wandb_flag': True, 'use_mse': True, 'use_scaler': False, 'fine-tuning': False, 'node_1': 128, 'node_2': 32, 'node_3': 8}
load the train data ...

 87%|████████▋ | 33542/38400 [00:03<00:00, 8970.42it/s]
total train data: 38400
load the valid data ...
100%|██████████| 38400/38400 [00:04<00:00, 8978.20it/s]
100%|██████████| 9600/9600 [00:01<00:00, 9088.33it/s]
not fine-tuning
Training_Loss At Epoch 0:	 0.384429
Testing_Loss At Epoch 0:	 0.376089
epoch0, time used: 1.21, lr: 0.001
Training_Loss At Epoch 1:	 0.366536
Testing_Loss At Epoch 1:	 0.374816
epoch1, time used: 0.75, lr: 0.001
Training_Loss At Epoch 2:	 0.366359
Testing_Loss At Epoch 2:	 0.374752
epoch2, time used: 0.67, lr: 0.001
epoch3, time used: 0.69, lr: 0.001
epoch4, time used: 0.72, lr: 0.001
Training_Loss At Epoch 5:	 0.366353
Testing_Loss At Epoch 5:	 0.374736
epoch5, time used: 0.76, lr: 0.001
epoch6, time used: 0.76, lr: 0.001
epoch7, time used: 0.77, lr: 0.001
epoch8, time used: 0.76, lr: 0.001
epoch9, time used: 0.76, lr: 0.001
epoch10, time used: 0.76, lr: 0.001
epoch11, time used: 0.77, lr: 0.001
epoch12, time used: 0.77, lr: 0.001
epoch13, time used: 0.77, lr: 0.001
epoch14, time used: 0.68, lr: 0.0001
epoch15, time used: 0.68, lr: 0.0001
epoch16, time used: 0.67, lr: 0.0001
epoch17, time used: 0.67, lr: 0.0001
epoch18, time used: 0.67, lr: 0.0001
epoch19, time used: 0.67, lr: 0.0001
epoch20, time used: 0.67, lr: 0.0001
epoch21, time used: 0.69, lr: 0.0001
epoch22, time used: 0.72, lr: 0.0001
epoch23, time used: 0.72, lr: 0.0001
epoch24, time used: 0.75, lr: 0.0001
epoch25, time used: 0.76, lr: 1e-05
epoch26, time used: 0.68, lr: 1e-05